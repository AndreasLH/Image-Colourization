{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import Image, clear_output, display\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "from skimage.color import lab2rgb, rgb2hsv, rgb2lab\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import Places365\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetBlock(nn.Module):\n",
    "    def __init__(self, nf, ni, submodule=None, input_c=None, dropout=False,\n",
    "                 innermost=False, outermost=False):\n",
    "        super().__init__()\n",
    "        self.outermost = outermost\n",
    "        if input_c is None: input_c = nf\n",
    "        downconv = nn.Conv2d(input_c, ni, kernel_size=4,\n",
    "                             stride=2, padding=1, bias=False)\n",
    "        downrelu = nn.LeakyReLU(0.2, True)\n",
    "        downnorm = nn.BatchNorm2d(ni)\n",
    "        uprelu = nn.ReLU(True)\n",
    "        upnorm = nn.BatchNorm2d(nf)\n",
    "        \n",
    "        if outermost:\n",
    "            upconv = nn.ConvTranspose2d(ni * 2, nf, kernel_size=4,\n",
    "                                        stride=2, padding=1)\n",
    "            down = [downconv]\n",
    "            up = [uprelu, upconv, nn.Tanh()]\n",
    "            model = down + [submodule] + up\n",
    "        elif innermost:\n",
    "            upconv = nn.ConvTranspose2d(ni, nf, kernel_size=4,\n",
    "                                        stride=2, padding=1, bias=False)\n",
    "            down = [downrelu, downconv]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "            model = down + up\n",
    "        else:\n",
    "            upconv = nn.ConvTranspose2d(ni * 2, nf, kernel_size=4,\n",
    "                                        stride=2, padding=1, bias=False)\n",
    "            down = [downrelu, downconv, downnorm]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "            if dropout: up += [nn.Dropout(0.5)]\n",
    "            model = down + [submodule] + up\n",
    "        self.model = nn.Sequential(*model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.outermost:\n",
    "            return self.model(x)\n",
    "        else:\n",
    "            return torch.cat([x, self.model(x)], 1)\n",
    "        \n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, input_c=1, output_c=2, n_down=8, num_filters=64):\n",
    "        super().__init__()\n",
    "        unet_block = UnetBlock(num_filters * 8, num_filters * 8, innermost=True)\n",
    "        for _ in range(n_down - 5):\n",
    "            unet_block = UnetBlock(num_filters * 8, num_filters * 8, submodule=unet_block, dropout=True)\n",
    "        out_filters = num_filters * 8\n",
    "        for _ in range(3):\n",
    "            unet_block = UnetBlock(out_filters // 2, out_filters, submodule=unet_block)\n",
    "            out_filters //= 2\n",
    "        self.model = UnetBlock(output_c, out_filters, input_c=input_c, submodule=unet_block, outermost=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unet(\n",
      "  (model): UnetBlock(\n",
      "    (model): Sequential(\n",
      "      (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): UnetBlock(\n",
      "        (model): Sequential(\n",
      "          (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): UnetBlock(\n",
      "            (model): Sequential(\n",
      "              (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "              (1): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "              (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (3): UnetBlock(\n",
      "                (model): Sequential(\n",
      "                  (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                  (1): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                  (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (3): UnetBlock(\n",
      "                    (model): Sequential(\n",
      "                      (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                      (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                      (3): UnetBlock(\n",
      "                        (model): Sequential(\n",
      "                          (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                          (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                          (3): UnetBlock(\n",
      "                            (model): Sequential(\n",
      "                              (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                              (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                              (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                              (3): UnetBlock(\n",
      "                                (model): Sequential(\n",
      "                                  (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                                  (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                                  (2): ReLU(inplace=True)\n",
      "                                  (3): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                                  (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                                )\n",
      "                              )\n",
      "                              (4): ReLU(inplace=True)\n",
      "                              (5): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                              (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                              (7): Dropout(p=0.5, inplace=False)\n",
      "                            )\n",
      "                          )\n",
      "                          (4): ReLU(inplace=True)\n",
      "                          (5): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                          (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                          (7): Dropout(p=0.5, inplace=False)\n",
      "                        )\n",
      "                      )\n",
      "                      (4): ReLU(inplace=True)\n",
      "                      (5): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                      (7): Dropout(p=0.5, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (4): ReLU(inplace=True)\n",
      "                  (5): ConvTranspose2d(1024, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                  (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                )\n",
      "              )\n",
      "              (4): ReLU(inplace=True)\n",
      "              (5): ConvTranspose2d(512, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "              (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (4): ReLU(inplace=True)\n",
      "          (5): ConvTranspose2d(256, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): ConvTranspose2d(128, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (4): Tanh()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "u = Unet(input_c=1, output_c=2, n_down=8, num_filters=64)\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2, 256, 256])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_input = torch.randn(16, 1, 256, 256) # batch_size, channels, size, size\n",
    "out = u(dummy_input)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetBlock(nn.Module):\n",
    "    def __init__(self, nf, ni, submodule=None, input_c=None, dropout=False,\n",
    "                 innermost=False, outermost=False, down=True):\n",
    "        super().__init__()\n",
    "        self.outermost = outermost\n",
    "        self.innermost = innermost\n",
    "        self.down_sample = down\n",
    "        if input_c is None: input_c = nf\n",
    "        downconv = nn.Conv2d(input_c, ni, kernel_size=4,\n",
    "                             stride=2, padding=1, bias=False)\n",
    "        downrelu = nn.LeakyReLU(0.2, True)\n",
    "        uprelu = nn.ReLU(True)\n",
    "        downnorm = nn.BatchNorm2d(ni)\n",
    "        upnorm = nn.BatchNorm2d(nf)\n",
    "        \n",
    "        if outermost:\n",
    "            upconv = nn.ConvTranspose2d(ni * 2, nf, kernel_size=4,\n",
    "                                        stride=2, padding=1)\n",
    "            if down:\n",
    "                down = [downconv]\n",
    "                model = down + [submodule]\n",
    "            else:\n",
    "                up = [uprelu, upconv, nn.Tanh()]\n",
    "                model = [submodule] + up\n",
    "        \n",
    "        elif innermost:\n",
    "            upconv = nn.ConvTranspose2d(ni, nf, kernel_size=4,\n",
    "                                        stride=2, padding=1, bias=False)\n",
    "  \n",
    "            down = [downrelu, downconv]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "            model = down + up\n",
    "        else:\n",
    "            upconv = nn.ConvTranspose2d(ni * 2, nf, kernel_size=4,\n",
    "                                        stride=2, padding=1, bias=False)\n",
    "            if down:\n",
    "                down = [downrelu, downconv, downnorm]\n",
    "                model = down + [submodule]\n",
    "            else:\n",
    "                up = [uprelu, upconv, upnorm]\n",
    "                if dropout: up += [nn.Dropout(0.5)]\n",
    "                if submodule is None:\n",
    "                    model = up\n",
    "                else:\n",
    "                    model = [submodule] + up\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.outermost:\n",
    "            print(\"outermost\")\n",
    "            print(\"x dim:\", x.shape)\n",
    "            y = self.model(x)\n",
    "            print(\"y dim:\", y.shape)\n",
    "            return y\n",
    "        else:\n",
    "            print(\"another layer\")\n",
    "            print(\"x dim:\",x.shape)\n",
    "            y = torch.cat([x, self.model(x)],1)\n",
    "#             y = self.model(x)\n",
    "            print(\"y dim:\",y.shape)\n",
    "            return y\n",
    "        \n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, input_c=1, output_c=2, n_down=8, num_filters=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        # down sampling + bottleneck\n",
    "        unet_block = UnetBlock(num_filters * 8, num_filters*8, innermost=True) # bottleneck\n",
    "        for _ in range(n_down - 5): # build 3 first downsampling layers\n",
    "            unet_block = UnetBlock(num_filters * 8, num_filters * 8, submodule=unet_block, dropout=True, down=True)\n",
    "        out_filters = num_filters * 8\n",
    "        for _ in range(3):\n",
    "            unet_block = UnetBlock(out_filters // 2, out_filters, submodule=unet_block, down=True)\n",
    "            out_filters //= 2\n",
    "        unet_block = UnetBlock(output_c, out_filters, input_c=input_c, submodule=unet_block, outermost=True, down=True)\n",
    "        self.model = unet_block\n",
    "    \n",
    "        # upsampling\n",
    "#         unet_block = UnetBlock(num_filters * 8, num_filters * 8, dropout=True, down=False) # first upsampling layer\n",
    "#         for _ in range(n_down - 6): # NOTE: we loop one time less here since we have already defined the first upsampling layer\n",
    "#             unet_block = UnetBlock(num_filters * 8, num_filters * 8, submodule = unet_block, dropout=True, down=False)\n",
    "#         out_filters = num_filters * 8\n",
    "#         for _ in range(3):\n",
    "#             unet_block = UnetBlock(out_filters // 2, out_filters, submodule=unet_block, down=False)\n",
    "#             out_filters //= 2\n",
    "#         unet_block = UnetBlock(output_c, out_filters, input_c=input_c, submodule=unet_block, outermost=True, down=False)\n",
    "        \n",
    "        # FFNN for label\n",
    "        self.embed = nn.Embedding(365,100)\n",
    "#         self.linear = nn.Sequential()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "#         embedding = self.embed(self.label)\n",
    "#         encoded_label = self.linear(embedding)\n",
    "#         x = torch.cat([x, encoded_label],dim=1) # concatenate label and data in dim 1\n",
    "#         x = self.up(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unet(\n",
      "  (model): UnetBlock(\n",
      "    (model): Sequential(\n",
      "      (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): UnetBlock(\n",
      "        (model): Sequential(\n",
      "          (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): UnetBlock(\n",
      "            (model): Sequential(\n",
      "              (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "              (1): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "              (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (3): UnetBlock(\n",
      "                (model): Sequential(\n",
      "                  (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                  (1): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                  (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  (3): UnetBlock(\n",
      "                    (model): Sequential(\n",
      "                      (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                      (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                      (3): UnetBlock(\n",
      "                        (model): Sequential(\n",
      "                          (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                          (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                          (3): UnetBlock(\n",
      "                            (model): Sequential(\n",
      "                              (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                              (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                              (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                              (3): UnetBlock(\n",
      "                                (model): Sequential(\n",
      "                                  (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "                                  (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                                  (2): ReLU(inplace=True)\n",
      "                                  (3): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                                  (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                                )\n",
      "                              )\n",
      "                            )\n",
      "                          )\n",
      "                        )\n",
      "                      )\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (embed): Embedding(365, 100)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "u = Unet(input_c=1, output_c=2, n_down=8, num_filters=64)\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(16, 1, 256, 256) # batch_size, channels, size, size\n",
    "out = u(dummy_input)\n",
    "out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
